{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecf8a8a6",
   "metadata": {},
   "source": [
    "# Integrated Gradients\n",
    "\n",
    "Our goal is to understand how a deep learning model makes its decisions by writing the XAI code ourselves. We will implement and compare three fundamental methods: Vanilla Gradients,  Gradient x Input, and Integrated Gradients.\n",
    "We will use a pre-trained Inception V3 model from PyTorch's torchvision library and a dataset of images. This model was trained on the large ImageNet dataset and can classify images into 1000 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a47c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "\n",
    "import utils # set of helper functions\n",
    "import torch # pytorch for deep learning\n",
    "import numpy as np # numpy for matrix operations\n",
    "import matplotlib.pyplot as plt # matplotlib for plotting\n",
    "from utils import custom_cmap # custom color map for visualizing attributions\n",
    "from captum.attr import LayerGradCam, Occlusion, IntegratedGradients # captum built in attribution methods\n",
    "from torchvision.models import inception_v3 # torchvision for loading pre-trained models\n",
    "import warnings # to suppress warnings\n",
    "warnings.filterwarnings(\"ignore\") # ignore warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3194ccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained inception_v3 model and set it to evaluation mode\n",
    "model = inception_v3(pretrained=True)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ec26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images \n",
    "images = np.load('../data/images.npy') # images are 299 by 299 and normalize dbetween -1 and 1\n",
    "# Load corresponding labels\n",
    "labels = np.load('../data/labels.npy')\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441b52f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all normalized images using utils function so the images look good\n",
    "fig, axs = plt.subplots(1, images.shape[0], figsize=(5 * images.shape[0], 5))\n",
    "for i in range(images.shape[0]):\n",
    "    axs[i].imshow(utils.normalize(images[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2792db10",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 10px;\">\n",
    "\n",
    "```python\n",
    "# You can use the following pattern to plot your attribution results for all images:\n",
    "\n",
    "fig, axs = plt.subplots(1, images.shape[0], figsize=(5 * images.shape[0], 5)) \n",
    "\n",
    "for i, image in enumerate(images):\n",
    "\n",
    "    # Convert your numpy image to a PyTorch tensor of shape (1, C, H, W)\n",
    "    input_tensor = torch.tensor(image).permute(2, 0, 1).unsqueeze(0).float()\n",
    "\n",
    "    # The target class label for the corresponding image\n",
    "    target_class = int(labels[i])\n",
    "   \n",
    "    # Your function or library call here to compute the saliency map\n",
    "    #-----------------------------------------------------------------\n",
    "\n",
    "    saliency_map = your_attribution_method(model, target_label)\n",
    "\n",
    "    #-----------------------------------------------------------------\n",
    "\n",
    "    # Turn into numpy array and detach if necessary\n",
    "\n",
    "    # Average over color channels if necessary\n",
    "\n",
    "    # plot original image if necessary and saliency_map\n",
    "    axs[i].imshow(utils.normalize(image))\n",
    "    axs[i].imshow(utils.normalize(saliency_map), cmap='some color map', alpha=0.3)\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565c4a05",
   "metadata": {},
   "source": [
    "### 1. Vanilla Gradients \n",
    "\n",
    "Over here I have curated a simple example of how gradients are calculated in PyTorch. Please carefully read through the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784fba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_gradients(model, input_tensor, target_class):\n",
    "    \"\"\"\n",
    "    Calculates Vanilla Gradients (also known as Saliency Maps).\n",
    "    \n",
    "    Args:\n",
    "        --> model: Your pre-trained PyTorch model.\n",
    "        --> input_tensor: The image you want to explain (as a PyTorch tensor). In our case one of the four images.\n",
    "        --> target_class: The class you want to explain (e.g., the index for \"cat\"). In our case the associated label.\n",
    "\n",
    "    Returns:\n",
    "        --> A NumPy array containing the raw gradients of each pixel.\n",
    "    \"\"\"\n",
    "    \n",
    "    # This is a very important command in PyTorch.\n",
    "    # It tells the model, \"We are now testing, not training.\" \n",
    "    # This disables certain layers like Dropout that behave differently\n",
    "    # during training and testing, ensuring our results are consistent \n",
    "    # and reflect how the model actually makes predictions.\n",
    "    model.eval()\n",
    "\n",
    "    # Here we tell PyTorch to \"Keep track of every calculation that involves this input_tensor.\n",
    "    # Since we will need to calculate gradients with respect to it later. \n",
    "    # Without this line, PyTorch wouldn't store the necessary information to compute the gradients.\n",
    "    input_tensor.requires_grad_(True)\n",
    "\n",
    "    # This is a standard prediction step. \n",
    "    # We pass our image through the model to get the output. \n",
    "    # The output is a list of scores for all possible classes (e.g., 1000 scores for ImageNet). \n",
    "    # The if statement is just a safety check because some models (like InceptionV3) return a tuple of outputs, \n",
    "    # and we only want the main prediction scores.\n",
    "    output = model(input_tensor)\n",
    "    if isinstance(output, tuple):\n",
    "        output = output[0] # Shape (1, num_classes)\n",
    "\n",
    "    # From the 1000 scores in output, we only care about one:\n",
    "    # the score for our target_class (e.g., \"whale\"). This line plucks out that single number.\n",
    "    #  output[0, ...] is used because our input_tensor has a batch dimension of 1\n",
    "    # so we select the first (and only) item in the batch.\n",
    "    score = output[0, target_class] # Shape (1,) this is a logit? Is this number bound? WHy do we make this choice?\n",
    "\n",
    "    # It's good practice to clear any old, leftover gradients. Think of it as erasing a chalkboard before a new calculation.\n",
    "    model.zero_grad()\n",
    "\n",
    "     # This single command is the heart of PyTorch's automatic differentiation. \n",
    "     # It triggers the backpropagation algorithm. It calculates the gradient of\n",
    "     # our score with respect to every single tensor in the computation graph that\n",
    "     # has requires_grad=True set. Since we set it on our input_tensor, PyTorch calculates\n",
    "     # the gradient of the 'whale' score with respect to every pixel in the input image.\n",
    "    score.backward()\n",
    "\n",
    "    # After score.backward() runs, PyTorch automatically populates the .grad attribute\n",
    "    # of our input tensor with the gradients we just calculated.\n",
    "    gradients = input_tensor.grad.clone()\n",
    "\n",
    "    # This is a standard cleanup chain to make the result usable outside of PyTorch:\n",
    "    saliency_map = gradients.cpu().detach().numpy()\n",
    "\n",
    "    return saliency_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1910e2db",
   "metadata": {},
   "source": [
    "Apply the above function to each image in our four image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af659873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot integrated gradients reults for all images\n",
    "fig, axs = plt.subplots(1, images.shape[0], figsize=(5 * images.shape[0], 5))\n",
    "for i, image in enumerate(images):\n",
    "    # Convert your numpy image to a PyTorch tensor\n",
    "    input_tensor = torch.tensor(image).permute(2, 0, 1).unsqueeze(0).float()\n",
    "    target_class = int(labels[i])\n",
    "    # Compute Vanilla Gradients saliency map\n",
    "    saliency_map = vanilla_gradients(model, input_tensor, target_class).squeeze()\n",
    "    # avarage over color channels\n",
    "    saliency_map = np.mean(saliency_map, axis=0)\n",
    "    # Plot integrated gradients saliency map \n",
    "    axs[i].imshow(saliency_map, cmap=custom_cmap, alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8083c0f0",
   "metadata": {},
   "source": [
    "Can you explain why it is so noisy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a165c15d",
   "metadata": {},
   "source": [
    "### 2. Turn vanilla gradients into Gradient x Input\n",
    "\n",
    "Vanilla gradients tell us which pixels are sensitive, but not how much each pixel actually contributes. To get a first-order approximation of a pixel's importance, we multiply the pixel's sensitivity by its actual magnitude. This gives us an estimate of how much it affects the output. In the following code, inside the for loop, implement Gradient × Input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30f879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot integrated gradients reults for all images\n",
    "fig, axs = plt.subplots(1, images.shape[0], figsize=(5 * images.shape[0], 5))\n",
    "for i, image in enumerate(images):\n",
    "    # Convert your numpy image to a PyTorch tensor\n",
    "    input_tensor = torch.tensor(image).permute(2, 0, 1).unsqueeze(0).float()\n",
    "    target_class = int(labels[i])\n",
    "    # Compute Gradient x Input saliency map\n",
    "\n",
    "    YOUR CODE HERE\n",
    "\n",
    "    # avarage over color channels\n",
    "    saliency_map = np.mean(saliency_map, axis=0)\n",
    "    # Plot integrated gradients saliency map \n",
    "    axs[i].imshow(saliency_map, cmap=custom_cmap, alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e27bfa",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>⚠️ Click here for the solution (this will use your \"solution pass\")</summary>\n",
    "  \n",
    "  ```python\n",
    "# Plot integrated gradients reults for all images\n",
    "fig, axs = plt.subplots(1, images.shape[0], figsize=(5 * images.shape[0], 5))\n",
    "for i, image in enumerate(images):\n",
    "    # Convert your numpy image to a PyTorch tensor\n",
    "    input_tensor = torch.tensor(image).permute(2, 0, 1).unsqueeze(0).float()\n",
    "    target_class = int(labels[i])\n",
    "    # Compute Gradients x Input saliency map\n",
    "    grads = vanilla_gradients(model, input_tensor, target_class).squeeze()\n",
    "    # Here is where we turn vanilla gradients into gradient x input\n",
    "    saliency_map = grads * input_tensor.squeeze().cpu().detach().numpy() # get rid of batch dimension, make numpy\n",
    "    # avarage over color channels\n",
    "    saliency_map = np.mean(saliency_map, axis=0)\n",
    "    # Plot integrated gradients saliency map \n",
    "    axs[i].imshow(saliency_map, cmap=custom_cmap, alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98324c95",
   "metadata": {},
   "source": [
    "### 3. Turn Gradient x Input into Integrated Gradients  \n",
    "Gradient x Input assumes a strong baseline (0) and is still noisy. You will not use your theoretical knowledge to write a function for Integrated gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604ec8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrated_gradients(model, input_tensor, target_class, baseline=None, steps=50):\n",
    "    \n",
    "    # Step 1: Set up the \"Path\" from Baseline to Input\n",
    "    # If no baseline is provided, create a tensor of zeros with the same shape\n",
    "    # as our input. For an image, this is like starting with a black image.\n",
    "    if baseline is None:\n",
    "        baseline = torch.zeros_like(input_tensor)\n",
    "    \n",
    "    # Create the sequence of interpolated inputs. This is our \"path\".\n",
    "    # We generate a series of images that gradually transition from the black\n",
    "    # baseline image to the final input image.\n",
    "    # The alpha term in our equation can be written as (float(i) / steps) and creates a fraction from 0.0 to 1.0.\n",
    "    # At i=0, scaled_input = baseline.\n",
    "    # At i=steps, scaled_input = input_tensor.\n",
    "    # You might want to test your code by plotting these scaled_inputs to ensure they transition correctly.\n",
    "    # Remember to set requires_grad=True so we can compute gradients with respect to these tensors later.\n",
    "    scaled_inputs = [ YOUR CODE HERE ]\n",
    "\n",
    "\n",
    "    # Step 2: Calculate Gradients at each Step Along the Path\n",
    "    # Store the gradients we collect at each step.\n",
    "    grads = []\n",
    "    for scaled_input in scaled_inputs:\n",
    "        # Essential step: clear any previous gradients.\n",
    "        model.zero_grad()\n",
    "\n",
    "        YOUR CODE HERE\n",
    "\n",
    "\n",
    "    # Step 3: Integrate the Gradients to Get Final Attributions\n",
    "    # The list of gradients is converted to a single NumPy array for easy calculation.\n",
    "    grads = np.array(grads)\n",
    "    \n",
    "    # To approximate the integral (the \"summing up\" part), we use the Trapezoidal rule.\n",
    "    # It's a more accurate way of averaging the gradients along the path than a simple mean.\n",
    "    # It works by averaging the gradients of adjacent pairs of steps.\n",
    "    # grads[:-1] is all gradients except the last one.\n",
    "    # grads[1:] is all gradients except the first one.\n",
    "    # This aligns them so we can average step 0 with 1, 1 with 2, etc.\n",
    "    avg_grads = YOUR CODE HERE\n",
    "    \n",
    "    # Take the average of these trapezoidal-approximated gradients across all steps.\n",
    "    # This gives us our final \"average sensitivity\" across the whole path.\n",
    "    average_gradients = np.mean(avg_grads, axis=0)\n",
    "\n",
    "\n",
    "    # Step 4: The core IG formula: Attribution = (Input - Baseline) * Integrated_Gradient\n",
    "    # We multiply the difference between our original image and the baseline\n",
    "    # by the average gradients we just calculated. This scales the attributions appropriately.\n",
    "    integrated_grads = YOUR CODE HERE\n",
    "\n",
    "    return integrated_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db91b1a",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>⚠️ Click here for the solution (this will use your \"solution pass\")</summary>\n",
    "  \n",
    "  ```python\n",
    "def integrated_gradients(model, input_tensor, target_class, baseline=None, steps=50):\n",
    "    \n",
    "    # Step 1: Set up the \"Path\" from Baseline to Input\n",
    "    # If no baseline is provided, create a tensor of zeros with the same shape\n",
    "    # as our input. For an image, this is like starting with a black image.\n",
    "    if baseline is None:\n",
    "        baseline = torch.zeros_like(input_tensor)\n",
    "    \n",
    "    # Create the sequence of interpolated inputs. This is our \"path\".\n",
    "    # We generate a series of images that gradually transition from the black\n",
    "    # baseline image to the final input image.\n",
    "    # The alpha term in our equation can be written as (float(i) / steps) and creates a fraction from 0.0 to 1.0.\n",
    "    # At i=0, scaled_input = baseline.\n",
    "    # At i=steps, scaled_input = input_tensor.\n",
    "    # You might want to test your code by plotting these scaled_inputs to ensure they transition correctly.\n",
    "    scaled_inputs = [\n",
    "        (baseline + (float(i) / steps) * (input_tensor - baseline)).requires_grad_(True) \n",
    "        for i in range(0, steps + 1)\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Step 2: Calculate Gradients at each Step Along the Path\n",
    "    # Store the gradients we collect at each step.\n",
    "    grads = []\n",
    "    for scaled_input in scaled_inputs:\n",
    "        # Essential step: clear any previous gradients.\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Get the model's prediction for the current interpolated input.\n",
    "        output = model(scaled_input)\n",
    "        \n",
    "        # We are interested in the score for our specific target class.\n",
    "        # This score is what we will measure the sensitivity (gradient) of.\n",
    "        # We extract the logit/score for our target class.\n",
    "        score_for_target_class = output[0, target_class]\n",
    "        \n",
    "        # Calculate the gradients of the target class score with respect to the input pixels.\n",
    "        # This tells us: \"For this specific step, which pixels would I need to change\n",
    "        # to increase the 'cat' score the most?\"\n",
    "        score_for_target_class.backward()\n",
    "        \n",
    "        # .grad contains the calculated gradients.\n",
    "        # .detach() prevents PyTorch from tracking this operation further.\n",
    "        # .cpu().numpy() moves the data to the CPU and converts it to a NumPy array.\n",
    "        grads.append(scaled_input.grad.detach().cpu().numpy())\n",
    "        \n",
    "\n",
    "    # Step 3: Integrate the Gradients to Get Final Attributions\n",
    "    # The list of gradients is converted to a single NumPy array for easy calculation.\n",
    "    grads = np.array(grads)\n",
    "    \n",
    "    # To approximate the integral (the \"summing up\" part), we use the Trapezoidal rule.\n",
    "    # It's a more accurate way of averaging the gradients along the path than a simple mean.\n",
    "    # It works by averaging the gradients of adjacent pairs of steps.\n",
    "    # grads[:-1] is all gradients except the last one.\n",
    "    # grads[1:] is all gradients except the first one.\n",
    "    # This aligns them so we can average step 0 with 1, 1 with 2, etc.\n",
    "    avg_grads = (grads[:-1] + grads[1:]) / 2.0\n",
    "    \n",
    "    # Take the average of these trapezoidal-approximated gradients across all steps.\n",
    "    # This gives us our final \"average sensitivity\" across the whole path.\n",
    "    average_gradients = np.mean(avg_grads, axis=0)\n",
    "\n",
    "\n",
    "    # Step 4: The core IG formula: Attribution = (Input - Baseline) * Integrated_Gradient\n",
    "    # We multiply the difference between our original image and the baseline\n",
    "    # by the average gradients we just calculated. This scales the attributions appropriately.\n",
    "    integrated_grads = (input_tensor.cpu().detach().numpy() - baseline.cpu().detach().numpy()) * average_gradients\n",
    "\n",
    "    return integrated_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780c8f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot integrated gradients reults for all images\n",
    "fig, axs = plt.subplots(1, images.shape[0], figsize=(5 * images.shape[0], 5))\n",
    "for i, image in enumerate(images):\n",
    "    # Convert your numpy image to a PyTorch tensor\n",
    "    input_tensor = torch.tensor(image).permute(2, 0, 1).unsqueeze(0).float()\n",
    "    target_class = int(labels[i])\n",
    "    # Compute Integrated Gradients saliency map\n",
    "    saliency_map = integrated_gradients(model, input_tensor, target_class).squeeze()\n",
    "    # avarage over color channels\n",
    "    saliency_map = np.mean(saliency_map, axis=0)\n",
    "    # Plot integrated gradients saliency map \n",
    "    axs[i].imshow(saliency_map, cmap=custom_cmap, alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc14506",
   "metadata": {},
   "source": [
    "### 4. Change the baseline and comment on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570437da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a different baseline of all ones\n",
    "fig, axs = plt.subplots(1, images.shape[0], figsize=(5 * images.shape[0], 5))\n",
    "for i, image in enumerate(images):\n",
    "    # Convert your numpy image to a PyTorch tensor\n",
    "    input_tensor = torch.tensor(image).permute(2, 0, 1).unsqueeze(0).float()\n",
    "    target_class = int(labels[i])\n",
    "    # Compute Integrated Gradients saliency map\n",
    "    saliency_map = integrated_gradients(model, input_tensor, target_class, baseline=torch.ones_like(input_tensor)).squeeze()\n",
    "    # avarage over color channels\n",
    "    saliency_map = np.mean(saliency_map, axis=0)\n",
    "    # Plot integrated gradients saliency map \n",
    "    axs[i].imshow(saliency_map, cmap=custom_cmap, alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82e0680",
   "metadata": {},
   "source": [
    "### 5. Compare your results by implementing Captum's IntegratedGradients method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa7e2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Integrated Gradients\n",
    "ig = IntegratedGradients(model)\n",
    "\n",
    "# Plot Integrated Gradients reults from Captum library for all images\n",
    "fig, axs = plt.subplots(1, images.shape[0], figsize=(5 * images.shape[0], 5))\n",
    "for i, image in enumerate(images):\n",
    "    # Convert your numpy image to a PyTorch tensor\n",
    "    input_tensor = torch.tensor(image).permute(2, 0, 1).unsqueeze(0).float()\n",
    "    target_class = int(labels[i])\n",
    "    # Compute Integrated Gradients saliency map\n",
    "    saliency_map = ig.attribute(input_tensor, target=target_class).squeeze().detach().numpy()\n",
    "    # avarage over color channels\n",
    "    saliency_map = np.mean(saliency_map, axis=0)\n",
    "    # Plot integrated gradients saliency map \n",
    "    axs[i].imshow(saliency_map, cmap=custom_cmap, alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5143c843",
   "metadata": {},
   "source": [
    "### 6. List some benefits and downsides of Integrated Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dbaecf",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>⚠️ Click here for the solution (this will use your \"solution pass\")</summary>\n",
    "\n",
    "Benifits\n",
    "+ Better baseline handling: Gradient × Input assumes a baseline of all zeros (black image). IG generalizes this: you choose a baseline (e.g., black image, blurred image, average image) and integrate the gradients along the straight path from baseline to input. This makes attributions less sensitive to arbitrary pixel values. \n",
    "+ Completeness property: The attributions add up (approximately) to the difference in output between the input and baseline. This gives a principled way to explain the model's decision. \n",
    "+ Noise reduction: By averaging gradients along many steps, IG smooths out the noise you often see in vanilla gradients. \n",
    "+ Handles saturation In ReLU networks: once activations saturate (flat regions), vanilla gradients can go to zero even if the feature was important. IG samples across the path, so it can capture the contribution before saturation.  \n",
    "\n",
    "Downsides\n",
    "- Choice of baseline: What is the “absence of signal” for an image? Black (all zeros)? Blurred? Mean of dataset? Different baselines can produce different attributions as you have seen. \n",
    "- Computational cost: You need many gradient evaluations (say, 50–200) per input to approximate the integral. This is more expensive than vanilla gradients or Gradient × Input (which only need one backward pass). \n",
    "- Still local and path-dependent: IG assumes a straight-line path from baseline to input. If the model behaves nonlinearly along other paths, IG might miss interactions. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cas_env_main (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
