{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feccc739",
   "metadata": {},
   "source": [
    "# Shapley Values\n",
    "\n",
    "In this notebook we will try to understand how a pre-trained model makes its predictions using the SHAP (SHapley Additive exPlanations) library. The Tree model was trained to predict survival probability of passengers on the titanic. We will analyze the model by generating several plots that combine Shapley values at different resolutions. Your job is to read the [documentation](https://shap.readthedocs.io/en/latest/index.html), and interpret each of the generated plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5e6500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary libraries\n",
    "\n",
    "import shap # For SHAP plots\n",
    "import joblib # For loading the model\n",
    "import pandas as pd # For data manipulation\n",
    "shap.initjs() # Initialize JavaScript for SHAP visualizations\n",
    "import warnings # Mute all errors\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc124474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "# This model is a scikit-learn pipeline that takes raw data and outputs a prediction.\n",
    "# The model predicts a survival score\n",
    "# where a higher score means a higher chance of survival.\n",
    "model = joblib.load('../models/titanic_model.pkl')\n",
    "\n",
    "# Load the data\n",
    "# We load the same data the model was trained on to analyze its behavior.\n",
    "# This dataset contains information about the passengers on the Titanic.\n",
    "# It does not include the survival outcome, however this target was used during training.\n",
    "data = pd.read_csv('../data/titanic_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c143eaf",
   "metadata": {},
   "source": [
    "### 2: Preparing the Data and Creating the SHAP Explainer\n",
    "\n",
    "The SHAP library needs two main things to explain a model's predictions:\n",
    "\n",
    "- The Model: The prediction logic we want to explain.\n",
    "\n",
    "- The Data: A set of examples to run through the model for explanation.\n",
    "\n",
    "We will use the modern shap.Explainer interface, which automatically selects the best explainer type, in this case the fast TreeExplainer behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6980fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use all our data for the explanation, but in practice, a representative\n",
    "# sample (e.g., 100-1000 rows) is often sufficient and much faster.\n",
    "X = data\n",
    "\n",
    "# 1. Create a SHAP explainer object\n",
    "# We pass the model and the data to the explainer.\n",
    "# The explainer learns the model's behavior based on the data provided.\n",
    "explainer = shap.Explainer(model, X)\n",
    "\n",
    "# 2. Calculate SHAP values\n",
    "# This computes the SHAP value for each feature for each prediction.\n",
    "# The output is a special 'Explanation' object that contains the values,\n",
    "# the original data, feature names, and more.\n",
    "shap_values = explainer(X)\n",
    "\n",
    "print(\"SHAP values have been calculated.\")\n",
    "print(shap_values.shape) # (num_samples, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb416584",
   "metadata": {},
   "source": [
    "### 3: Global Feature Importance - The \"Big Picture\"\n",
    "\n",
    "Global explanations help us understand the model's behavior as a whole, across all predictions. We can answer questions like, \"What features are most important to the model overall?\" This is a global explanation. This is the simplest way to see which features have the biggest impact on the model's predictions. \n",
    "\n",
    "***Bar plot***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077eb99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the Bar Plot\n",
    "YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40f1e12",
   "metadata": {},
   "source": [
    "Your interpretation here: Which features are most important and what information can we extract from the plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4ee4a8",
   "metadata": {},
   "source": [
    "***Beeswarm Plot*** (Summary Plot)\n",
    "\n",
    "The beeswarm plot is much richer than the bar plot. It shows not only the importance of each feature but also how the value of that feature affects the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80364034",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Generate the Beeswarm Plot\n",
    "YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433bd192",
   "metadata": {},
   "source": [
    "Your interpretation here: Explain what the horizontal position encodes as well as the color. Generate hypotheses about survival statistics based on certain features. What would be the best combination of features for survival, and what would be the worst?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4194ee97",
   "metadata": {},
   "source": [
    "### 4. Local Explanations - Explaining a Single Prediction\n",
    "\n",
    "We now zoom in and explain how the model arrived at its prediction for one specific person. This is called a local explanation.\n",
    "\n",
    "***Waterfall Plot***\n",
    "\n",
    "The waterfall plot breaks down a single prediction, showing how each feature's contribution moves the prediction from the \"base value\" to the final score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619ff10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explain the prediction for the first person in our dataset (at index 0)\n",
    "YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe8a313",
   "metadata": {},
   "source": [
    "Your interpretation here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5120fb",
   "metadata": {},
   "source": [
    "### 5: Feature Interaction Effects\n",
    "\n",
    "Sometimes, the effect of one feature depends on the value of another. This is something our ICE and PDP plots could not reveal, however, SHAP can help us uncover these relationships.\n",
    "\n",
    "***Dependence Plot*** (Scatter Plot)\n",
    "\n",
    "A dependence plot shows how the value of a single feature impacts its SHAP value. We can color it by another feature to discover interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e563d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the effect of 'Age' on the model's prediction.\n",
    "# SHAP will automatically color the dots by the feature that interacts most with 'Age'.\n",
    "YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8262e2",
   "metadata": {},
   "source": [
    "Your interpretation here: What do the horizontal axis and colors encode? What is the color bar on the right, and how do we measure feature dependence? In other words, what is the telltale sign of dependence? Generate a working hypothesis about the plot's overall meaning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cas_env_main (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
