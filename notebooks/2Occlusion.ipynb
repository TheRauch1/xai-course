{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Occlusion\n",
    "\n",
    "In this exercise you will implement a *vanilla occlusion* explainer for images.\n",
    "\n",
    "**Goals** (20 minutes)\n",
    "1. Load a pre-trained classifier (Inception‑v4) and a small image set (provided as `.npy`)\n",
    "2. Load the data and preview\n",
    "3. Select an image from the four image dataset and compute the unmasked prediction\n",
    "4. Implement a **sliding window** mask with configurable *window size*, *stride*, and *baseline value*.\n",
    "5. For each masked variant, run the model and record the target-class probability and its **distance** from the unmasked probability.\n",
    "6. Visualize: a grid of masked images + scores\n",
    "7. Generate saliency map\n",
    "8. Change the kernal size and report obsevations\n",
    "9. Change the basline and report obsevations\n",
    "10. Compare results to the open source Captum solution\n",
    "\n",
    "> Fill in each `# YOUR CODE HERE` with the missing lines. Hints provided inline. Small asserts help you verify shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "\n",
    "import utils # set of helper functions\n",
    "import torch # pytorch for deep learning\n",
    "import cv2 # opencv for image manipulation\n",
    "import numpy as np # numpy for matrix operations\n",
    "from timm import create_model # timm for loading pre-trained models\n",
    "import matplotlib.pyplot as plt # matplotlib for plotting\n",
    "from utils import custom_cmap # custom color map for visualizing attributions\n",
    "from captum.attr import Occlusion # captum built in attribution methods\n",
    "from torchvision.models import inception_v3 # torchvision for loading pre-trained models\n",
    "import warnings # to suppress warnings\n",
    "warnings.filterwarnings(\"ignore\") # ignore warnings\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # set device to GPU if available, otherwise CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Load model (Inception‑v3, ImageNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained inception_v3 model and set it to evaluation mode\n",
    "model = inception_v3(pretrained=True)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Load data and preview\n",
    "\n",
    "Assume `.npy` files with shapes: `images: (N, 299, 299, 3)` in **[-1, 1]**, `labels: (N,)` class indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images \n",
    "images = np.load('../data/images.npy') # images are 299 by 299 and normalize dbetween -1 and 1\n",
    "# Load corresponding labels\n",
    "labels = np.load('../data/labels.npy')\n",
    "print(images.shape, labels.shape) # (num_images, H, W, channel) (num_images,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964e37b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all normalized images using utils function so the images look good\n",
    "fig, axs = plt.subplots(1, images.shape[0], figsize=(5 * images.shape[0], 5))\n",
    "for i in range(images.shape[0]):\n",
    "    axs[i].imshow(utils.normalize(images[i]))\n",
    "    axs[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "# Index 1 = Finch, Index 2 = pencil, Index 3 = bird, Index 4 = whale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3) Select an image and compute the **unmasked** prediction\n",
    "\n",
    "- Convert image HWC → CHW tensor in **(-1,1)**\n",
    "- Compute softmax probabilities and note the **top‑1** class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83fb362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select an Image\n",
    "\n",
    "image_index = 1\n",
    "sample_image = images[image_index] # Shape (H, W, C)\n",
    "sample_label = labels[image_index] if len(labels) > image_index else \"Unknown\"\n",
    "\n",
    "# It's always a good idea to check the shape of our data!\n",
    "# The format is (Height, Width, Channels), often called HWC.\n",
    "print(f\"Original image shape: {sample_image.shape} -> (Height, Width, Channels)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc7fe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Pre-process the Image for PyTorch\n",
    "\n",
    "# Models don't work with images directly; they need them in a specific format called a 'tensor'.\n",
    "\n",
    "# Step 2a: Change layout from HWC to CHW\n",
    "# PyTorch models expect the channel dimension to be first: (Channels, Height, Width)\n",
    "image_chw = # YOUR CODE HERE\n",
    "print(f\"Shape after transpose: {image_chw.shape} -> (Channels, Height, Width)\")\n",
    "\n",
    "# Step 2b: Convert from NumPy array to a PyTorch Tensor\n",
    "image_tensor = # YOUR CODE HERE\n",
    "\n",
    "# Step 2c: Add a \"batch\" dimension\n",
    "# Models are designed to work on a 'batch' of images at once. \n",
    "# We add a new dimension to say \"this is a batch containing 1 image\".\n",
    "# Shape becomes: (Batch Size, Channels, Height, Width)\n",
    "image_batch = # YOUR CODE HERE\n",
    "print(f\"Shape after adding batch dimension: {image_batch.shape} -> (Batch, C, H, W)\")\n",
    "\n",
    "# Step 2d: Move the tensor to the correct device (CPU or GPU)\n",
    "input_tensor = image_batch.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5432696",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>⚠️ Click here for the solution (this will use your \"solution pass\")</summary>\n",
    "  \n",
    "  ```python\n",
    "# 2. Pre-process the Image for PyTorch\n",
    "\n",
    "# Models don't work with images directly; they need them in a specific format called a 'tensor'.\n",
    "\n",
    "# Step 2a: Change layout from HWC to CHW\n",
    "# PyTorch models expect the channel dimension to be first: (Channels, Height, Width)\n",
    "image_chw = np.transpose(sample_image, (2, 0, 1))\n",
    "print(f\"Shape after transpose: {image_chw.shape} -> (Channels, Height, Width)\")\n",
    "\n",
    "# Step 2b: Convert from NumPy array to a PyTorch Tensor\n",
    "image_tensor = torch.from_numpy(image_chw.astype(np.float32))\n",
    "\n",
    "# Step 2c: Add a \"batch\" dimension\n",
    "# Models are designed to work on a 'batch' of images at once. \n",
    "# We add a new dimension to say \"this is a batch containing 1 image\".\n",
    "# Shape becomes: (Batch Size, Channels, Height, Width)\n",
    "image_batch = image_tensor.unsqueeze(0)\n",
    "print(f\"Shape after adding batch dimension: {image_batch.shape} -> (Batch, C, H, W)\")\n",
    "\n",
    "# Step 2d: Move the tensor to the correct device (CPU or GPU)\n",
    "input_tensor = image_batch.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af60dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Make the Prediction \n",
    "\n",
    "# We use torch.no_grad() to tell PyTorch that we not training the model,\n",
    "# which saves memory and computation.\n",
    "with torch.no_grad():\n",
    "    # Get the raw model outputs (called 'logits').\n",
    "    # Logits are the raw, unnormalized scores output by the model.\n",
    "    # Shape should be (Batch Size, Num Classes)\n",
    "    logits = # YOUR CODE HERE\n",
    "    \n",
    "    # Convert logits to probabilities using the softmax function\n",
    "    # Shape should be (Batch Size, Num Classes)\n",
    "    probabilities = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8461f3",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>⚠️ Click here for the solution (this will use your \"solution pass\")</summary>\n",
    "  \n",
    "  ```python\n",
    "# 3. Make the Prediction \n",
    "\n",
    "# We use torch.no_grad() to tell PyTorch that we not training the model,\n",
    "# which saves memory and computation.\n",
    "with torch.no_grad():\n",
    "    # Get the raw model outputs (called 'logits').\n",
    "    # Logits are the raw, unnormalized scores output by the model.\n",
    "    # Shape should be (Batch Size, Num Classes)\n",
    "    logits = model(input_tensor) \n",
    "    \n",
    "    # Convert logits to probabilities using the softmax function\n",
    "    # Shape should be (Batch Size, Num Classes)\n",
    "    probabilities = torch.softmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b55136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Find the Top Class and its Probability\n",
    "\n",
    "# Find the class with the highest probability\n",
    "top_probability = # YOUR CODE HERE\n",
    "top_class_index = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e933d9e",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>⚠️ Click here for the solution (this will use your \"solution pass\")</summary>\n",
    "  \n",
    "  ```python\n",
    "# 4. Find the Top Class and its Probability\n",
    "\n",
    "# Find the class with the highest probability\n",
    "top_probability = probabilities.max()\n",
    "top_class_index = probabilities.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b35563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Interpret the Results\n",
    "# .item() is used to extract the single number from a tensor\n",
    "print(f\"\\n--- Prediction ---\")\n",
    "print(f\"Target Class (top-1): {top_class_index.item()}\")\n",
    "print(f\"Confidence: {top_probability.item():.4f}\")\n",
    "if sample_label != \"Unknown\":\n",
    "    print(f\"Actual Label: {sample_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Implement **sliding‑window** occlusion. \n",
    "Use this code block as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to CHW tensor\n",
    "def to_chw_tensor(img_hwc: np.ndarray) -> torch.Tensor:\n",
    "    # Convert HWC [-1,1] -> CHW\n",
    "    chw = np.transpose(img_hwc, (2, 0, 1)).astype(np.float32)\n",
    "    t = torch.from_numpy(chw).unsqueeze(0)  # [1,C,H,W]\n",
    "    return t\n",
    "\n",
    "# Sliding window occlusion utilities\n",
    "def sliding_window_positions(H: int, W: int, win: int, stride: int):\n",
    "    ys = list(range(0, max(1, H - win + 1), stride))\n",
    "    xs = list(range(0, max(1, W - win + 1), stride))\n",
    "    if ys[-1] != H - win:\n",
    "        ys.append(H - win)\n",
    "    if xs[-1] != W - win:\n",
    "        xs.append(W - win)\n",
    "    for y in ys:\n",
    "        for x in xs:\n",
    "            yield y, x\n",
    "\n",
    "def apply_mask(img_hwc: np.ndarray, y: int, x: int, win: int, baseline: float = 0.0):\n",
    "    out = img_hwc.copy()\n",
    "    out[y:y+win, x:x+win, :] = baseline\n",
    "    return out\n",
    "\n",
    "# Occlusion scan function \n",
    "# This function performs an occlusion scan on the input image using the specified model.\n",
    "# It slides a window across the image, applies a mask, and computes the model's predictions for each masked image. \n",
    "# It returns the results  wich includes the position of the window,\n",
    "# the predicted probability for the target class, and the absolute difference from the true probability.\n",
    "def occlusion_scan(model, img_hwc: np.ndarray, win: int = 50, stride: int = 50,\n",
    "                   baseline: float = 0.0, target_class: int = None, device=None):\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "\n",
    "    x0 = to_chw_tensor(img_hwc).to(device)\n",
    "    with torch.no_grad():\n",
    "        p0 = torch.softmax(model(x0), dim=-1)\n",
    "        if target_class is None:\n",
    "            target_class = int(p0.argmax(dim=-1))\n",
    "        p_true = float(p0[0, target_class])\n",
    "\n",
    "    results = []\n",
    "    for (y, x) in sliding_window_positions(img_hwc.shape[0], img_hwc.shape[1], win, stride):\n",
    "        masked = apply_mask(img_hwc, y, x, win, baseline=baseline)\n",
    "        xt = to_chw_tensor(masked).to(device)\n",
    "        with torch.no_grad():\n",
    "            p = torch.softmax(model(xt), dim=-1)[0, target_class].item()\n",
    "        results.append({'y': y, 'x': x, 'win': win, 'p': p, 'delta': abs(p - p_true), 'image': masked})\n",
    "    return results, target_class, p_true "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) For each masked variant, run the model and record the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIN = 100       # window size\n",
    "STRIDE = 100    # stride\n",
    "BASELINE = 0.0 # baseline\n",
    "\n",
    "results, tgt, p_true = occlusion_scan(\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "    \n",
    ")\n",
    "print(f'Occlusion produced {len(results)} masked variants. Target class = {tgt}, p_true = {p_true:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3741171",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>⚠️ Click here for the solution (this will use your \"solution pass\")</summary>\n",
    "  \n",
    "  ```python\n",
    "WIN = 100 # window size\n",
    "STRIDE = 100 # stride\n",
    "BASELINE = 0.0 # baseline\n",
    "\n",
    "results, tgt, p_true = occlusion_scan(model, sample_image, win=WIN, stride=STRIDE, baseline=BASELINE, target_class=None, device=device)\n",
    "print(f'Occlusion produced {len(results)} masked variants. Target class = {tgt}, p_true = {p_true:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1232166f",
   "metadata": {},
   "source": [
    "### 6) Visualize: a grid of masked images + scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236b28b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(results)\n",
    "cols = 5\n",
    "rows = int(np.ceil(n / cols))\n",
    "\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(3.2*cols, 3.2*rows))\n",
    "axs = np.array(axs).reshape(rows, cols)\n",
    "for ax in axs.ravel():\n",
    "    ax.axis('off')\n",
    "\n",
    "for i, r in enumerate(results):\n",
    "    ax = axs.ravel()[i]\n",
    "    ax.imshow((r['image'] + 1.0)/2.0)  # map [-1,1] -> [0,1]\n",
    "    ax.set_title(f\"p={r['p']:.2f}\\nΔ={r['delta']:.2f}\", fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(f'Occlusion (win={WIN}, stride={STRIDE}, baseline={BASELINE}) — target={tgt}, p_true={p_true:.2f}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afba9d2",
   "metadata": {},
   "source": [
    "### 7) Generate saliency map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441cdb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "W, H = sample_image.shape[1], sample_image.shape[0]\n",
    "\n",
    "grid_y = sorted(set([r['y'] for r in results]))\n",
    "grid_x = sorted(set([r['x'] for r in results]))\n",
    "delta_grid = np.zeros((len(grid_y), len(grid_x)), dtype=np.float32)\n",
    "for r in results:\n",
    "    yi = grid_y.index(r['y'])\n",
    "    xi = grid_x.index(r['x'])\n",
    "    delta_grid[yi, xi] = r['delta']\n",
    "\n",
    "# Normalize the heatmap for visualization\n",
    "heatmap = delta_grid.copy()\n",
    "heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-9)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(utils.normalize(sample_image), origin='upper')\n",
    "plt.imshow(\n",
    "    heatmap,\n",
    "    cmap='binary',\n",
    "    alpha=0.2,                    # stronger overlay\n",
    "    interpolation='nearest',      # blocky patches\n",
    "    extent=[0, W, H, 0],          # align heatmap grid to image pixels\n",
    "    vmin=0.0, vmax=1.0            # full dynamic range\n",
    ")\n",
    "plt.colorbar(label='|p_masked - p_true|', fraction=0.046, pad=0.04)\n",
    "plt.title('Occlusion Δ heatmap')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85ff848",
   "metadata": {},
   "source": [
    "### 8) Change the kernal size and report obsevations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ad82c9",
   "metadata": {},
   "source": [
    "### 9) Change the basline and report obsevations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Short questions (answer in text or as comments)\n",
    "\n",
    "1. What happens to the heatmap when you increase `WIN` (window size)? Why?\n",
    "\n",
    "2. What happens when you **decrease** `STRIDE`? How does it affect runtime and the smoothness of the heatmap?\n",
    "\n",
    "3. Is `BASELINE = 0.0` always a good choice?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5227f4",
   "metadata": {},
   "source": [
    "### 10) Compare results to the open source Captum solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3816d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Occlusion\n",
    "occlusion = # YOUR CODE HERE\n",
    "\n",
    "# Plot occlusion reults for all images\n",
    "fig, axs = plt.subplots(1, images.shape[0], figsize=(5 * images.shape[0], 5))\n",
    "for i, image in enumerate(images):\n",
    "    # Convert your numpy image to a PyTorch tensor\n",
    "    input_tensor = torch.tensor(image).permute(2, 0, 1).unsqueeze(0).float()\n",
    "    target_class = int(labels[i])\n",
    "    # Compute the occlusion saliency map\n",
    "    saliency_map = occlusion.attribute(\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    )\n",
    "    # Convert attributions to numpy for visualization\n",
    "    saliency_map = saliency_map.squeeze().detach().numpy()\n",
    "    # avarage over color channels\n",
    "    saliency_map = np.mean(saliency_map, axis=0)\n",
    "    # Plot integrated gradients saliency map \n",
    "    axs[i].imshow(utils.normalize(image))\n",
    "    axs[i].imshow(saliency_map, cmap='binary', alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9824079e",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>⚠️ Click here for the solution (this will use your \"solution pass\")</summary>\n",
    "  \n",
    "  ```python\n",
    "\n",
    "# Initialize Occlusion\n",
    "occlusion = Occlusion(model)\n",
    "\n",
    "# Plot occlusion reults for all images\n",
    "fig, axs = plt.subplots(1, images.shape[0], figsize=(5 * images.shape[0], 5))\n",
    "for i, image in enumerate(images):\n",
    "    # Convert your numpy image to a PyTorch tensor\n",
    "    input_tensor = torch.tensor(image).permute(2, 0, 1).unsqueeze(0).float()\n",
    "    target_class = int(labels[i])\n",
    "    # Compute the occlusion saliency map\n",
    "    saliency_map = occlusion.attribute(\n",
    "        input_tensor,\n",
    "        strides=(3, 110, 110),  # Strides for sliding window across channels, height, width\n",
    "        target=target_class,  # Target class index\n",
    "        sliding_window_shapes=(3, 110, 110),  # Window size for occlusion (channels, height, width)\n",
    "    )\n",
    "    # Convert attributions to numpy for visualization\n",
    "    saliency_map = saliency_map.squeeze().detach().numpy()\n",
    "    # avarage over color channels\n",
    "    saliency_map = np.mean(saliency_map, axis=0)\n",
    "    # Plot integrated gradients saliency map \n",
    "    axs[i].imshow(utils.normalize(image))\n",
    "    axs[i].imshow(saliency_map, cmap='binary', alpha=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cas_env (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
